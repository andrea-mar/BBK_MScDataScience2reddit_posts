{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic analysis \n",
    "unsupervised learning\n",
    "\n",
    "\n",
    "Vayansky and Kumar (2020): \n",
    "review topic modeling algorithms \n",
    "According to their guidelines:\n",
    "If average number of word per document > 50 and complext topic relationships are NOT of interest ( for ex: evolution of topics over time or correlation between topics), then Latent Dirichlet allocation (LDA) would be a good choice. \n",
    "\n",
    "\n",
    "\n",
    "<small>Vayansky, I., & Kumar, S. A. P. (2020) 'A review of topic modeling methods', Information Systems, 94, 101582.</small>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = 'data/english_posts_cleaned.csv'\n",
    "english_posts = pd.read_csv(file_path)\n",
    "english_posts.info()\n",
    "\n",
    "# 522377 posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>main_submission_id</th>\n",
       "      <th>comment_parent_id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>post_type</th>\n",
       "      <th>text</th>\n",
       "      <th>datetime</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>text_length</th>\n",
       "      <th>language</th>\n",
       "      <th>language_ft</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>is4ft9s</td>\n",
       "      <td>y2q46p</td>\n",
       "      <td>t3_y2q46p</td>\n",
       "      <td>autism</td>\n",
       "      <td>comment</td>\n",
       "      <td>I don t think it works like that</td>\n",
       "      <td>2022-10-13 05:58:56</td>\n",
       "      <td>10</td>\n",
       "      <td>2022</td>\n",
       "      <td>32</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is4gwqj</td>\n",
       "      <td>y2q46p</td>\n",
       "      <td>t3_y2q46p</td>\n",
       "      <td>autism</td>\n",
       "      <td>comment</td>\n",
       "      <td>I do we have handicap add on to our government...</td>\n",
       "      <td>2022-10-13 06:12:48</td>\n",
       "      <td>10</td>\n",
       "      <td>2022</td>\n",
       "      <td>189</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>is4c22w</td>\n",
       "      <td>y2q46p</td>\n",
       "      <td>t3_y2q46p</td>\n",
       "      <td>autism</td>\n",
       "      <td>comment</td>\n",
       "      <td>Hey u Starflarity thank you for your post at r...</td>\n",
       "      <td>2022-10-13 05:14:14</td>\n",
       "      <td>10</td>\n",
       "      <td>2022</td>\n",
       "      <td>458</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id main_submission_id comment_parent_id subreddit post_type  \\\n",
       "0  is4ft9s             y2q46p         t3_y2q46p    autism   comment   \n",
       "1  is4gwqj             y2q46p         t3_y2q46p    autism   comment   \n",
       "2  is4c22w             y2q46p         t3_y2q46p    autism   comment   \n",
       "\n",
       "                                                text             datetime  \\\n",
       "0                   I don t think it works like that  2022-10-13 05:58:56   \n",
       "1  I do we have handicap add on to our government...  2022-10-13 06:12:48   \n",
       "2  Hey u Starflarity thank you for your post at r...  2022-10-13 05:14:14   \n",
       "\n",
       "   month  year  text_length language language_ft  \n",
       "0     10  2022           32       en          en  \n",
       "1     10  2022          189       en          en  \n",
       "2     10  2022          458       en          en  "
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = english_posts.copy()\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA - Topic modeling\n",
    "\n",
    "Based on the Dirichlet distribution = a family of continuous multivariate probability distributions parameterized by a vector Alpha of positive reals. It is a multivariate generalization of the beta distribution,hence its alternative name of multivariate beta distribution (MBD). Dirichlet distributions are commonly used as prior distributions in Bayesian statistics, and in fact, the Dirichlet distribution is the conjugate prior of the categorical distribution and multinomial distribution. (https://en.wikipedia.org/wiki/Dirichlet_distribution)\n",
    "\n",
    "LDA was initially proposed by Blei et al. (2003) and is based on the following assumpitions:\n",
    "- documents with similar topics use similar groups of words\n",
    "- latent topics can then be found by searching for groups of words that frequently occur together in documents across the corpus\n",
    "- documents are probability distributions over latent topics\n",
    "- topics themselves are probability distributions over words\n",
    "\n",
    "\n",
    "expalanation of how it works: https://www.youtube.com/watch?v=be7Xd2Ntai8&ab_channel=AnalyticsExcellence\n",
    "\n",
    "<small>Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003) 'Latent Dirichlet Allocation', Journal of Machine Learning Research, 3, 993-1022.</small>\n",
    "\n",
    "How to choose optimal number of topics ( for LDA this needs to be chosen by us ):\n",
    "- Subject expertise: if you might have an idea of how many topics are present in your documents, choose the number acordingly.\n",
    "- Perplexity: metric commonly used to evaluate the performance of an LDA model. Lower perplexity values indicate better models. However, it's important to note that perplexity alone might not always reflect the interpretability of topic\n",
    "- Coherence: measures the interpretability of topic. Higher coherence scores generally indicate better-defined topics. \n",
    "- Visual analysis: manually inspect the topics generated the different model. If the topics make sense and are coherent, it's a good indication that the model is good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install import_ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "import import_ipynb\n",
    "import reddit_post_analysis\n",
    "\n",
    "# Access the variable from the first notebook\n",
    "stopwords = reddit_post_analysis.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s',\n",
       " 'm',\n",
       " 'go',\n",
       " 'u',\n",
       " 're',\n",
       " 'Ye',\n",
       " 'OP',\n",
       " 've',\n",
       " 'd',\n",
       " 'll',\n",
       " 'ok',\n",
       " 'ex',\n",
       " 'Oh',\n",
       " 'im',\n",
       " 'NT',\n",
       " 'bc',\n",
       " 'don t',\n",
       " 'doesn t',\n",
       " 'didn t',\n",
       " 'isn t',\n",
       " 'aren t',\n",
       " 'wasn t',\n",
       " 'wouldn t',\n",
       " 'won t',\n",
       " 'don',\n",
       " 'doesn',\n",
       " 'didn',\n",
       " 'isn',\n",
       " 'aren',\n",
       " 'wasn',\n",
       " 'wouldn',\n",
       " 'won',\n",
       " 'get',\n",
       " \"it's\",\n",
       " \"i'd\",\n",
       " \"aren't\",\n",
       " 'otherwise',\n",
       " 'her',\n",
       " 'which',\n",
       " \"you're\",\n",
       " \"they've\",\n",
       " \"doesn't\",\n",
       " 'than',\n",
       " 'however',\n",
       " 'my',\n",
       " \"i've\",\n",
       " 'into',\n",
       " \"who's\",\n",
       " 'their',\n",
       " \"she's\",\n",
       " 'an',\n",
       " 'and',\n",
       " 'com',\n",
       " 'any',\n",
       " 'under',\n",
       " \"he'd\",\n",
       " 'other',\n",
       " 'by',\n",
       " 'k',\n",
       " 'on',\n",
       " 'why',\n",
       " \"when's\",\n",
       " 'am',\n",
       " 'do',\n",
       " \"shouldn't\",\n",
       " \"hasn't\",\n",
       " \"shan't\",\n",
       " 'there',\n",
       " 'we',\n",
       " 'does',\n",
       " 'yourselves',\n",
       " \"she'll\",\n",
       " \"i'll\",\n",
       " \"you'll\",\n",
       " 'each',\n",
       " 'not',\n",
       " \"haven't\",\n",
       " 'i',\n",
       " 'them',\n",
       " 'so',\n",
       " \"they'll\",\n",
       " 'can',\n",
       " 'few',\n",
       " 'myself',\n",
       " 'yours',\n",
       " 'at',\n",
       " 'could',\n",
       " 'would',\n",
       " 'these',\n",
       " \"he'll\",\n",
       " \"how's\",\n",
       " 'as',\n",
       " \"can't\",\n",
       " 'being',\n",
       " \"we'd\",\n",
       " 'once',\n",
       " \"couldn't\",\n",
       " 'also',\n",
       " 'both',\n",
       " 'over',\n",
       " \"mustn't\",\n",
       " 'how',\n",
       " 'having',\n",
       " 'below',\n",
       " 'all',\n",
       " \"weren't\",\n",
       " 'out',\n",
       " \"didn't\",\n",
       " \"hadn't\",\n",
       " \"wasn't\",\n",
       " \"we've\",\n",
       " \"here's\",\n",
       " 'until',\n",
       " \"isn't\",\n",
       " 'only',\n",
       " \"you've\",\n",
       " 'they',\n",
       " 'during',\n",
       " 'for',\n",
       " 'very',\n",
       " 'its',\n",
       " 'same',\n",
       " 'just',\n",
       " \"don't\",\n",
       " \"they're\",\n",
       " 'if',\n",
       " 'himself',\n",
       " \"where's\",\n",
       " 'was',\n",
       " 'it',\n",
       " 'about',\n",
       " 'between',\n",
       " 'no',\n",
       " 'our',\n",
       " 'then',\n",
       " 'did',\n",
       " 'down',\n",
       " 'most',\n",
       " 'ever',\n",
       " 'have',\n",
       " 'more',\n",
       " \"why's\",\n",
       " 'your',\n",
       " 'she',\n",
       " 'further',\n",
       " 'or',\n",
       " 'should',\n",
       " \"we'll\",\n",
       " 'what',\n",
       " 'while',\n",
       " 'doing',\n",
       " \"that's\",\n",
       " 'else',\n",
       " \"there's\",\n",
       " 'yourself',\n",
       " 'to',\n",
       " 'such',\n",
       " 'hers',\n",
       " 'nor',\n",
       " 'ours',\n",
       " 'you',\n",
       " \"wouldn't\",\n",
       " 'he',\n",
       " 'a',\n",
       " 'again',\n",
       " 'of',\n",
       " \"we're\",\n",
       " 'through',\n",
       " 'before',\n",
       " 'from',\n",
       " 'like',\n",
       " \"let's\",\n",
       " 'own',\n",
       " 'herself',\n",
       " 'shall',\n",
       " 'against',\n",
       " 'because',\n",
       " 'after',\n",
       " 'http',\n",
       " 'with',\n",
       " 'r',\n",
       " 'this',\n",
       " 'the',\n",
       " 'were',\n",
       " 'that',\n",
       " 'above',\n",
       " \"he's\",\n",
       " 'be',\n",
       " \"won't\",\n",
       " 'those',\n",
       " \"i'm\",\n",
       " 'some',\n",
       " 'hence',\n",
       " 'who',\n",
       " 'too',\n",
       " 'his',\n",
       " 'up',\n",
       " 'is',\n",
       " 'whom',\n",
       " 'him',\n",
       " 'are',\n",
       " 'where',\n",
       " 'cannot',\n",
       " 'ought',\n",
       " 'ourselves',\n",
       " 'therefore',\n",
       " 'since',\n",
       " 'in',\n",
       " 'here',\n",
       " 'been',\n",
       " 'itself',\n",
       " 'theirs',\n",
       " 'me',\n",
       " 'has',\n",
       " \"she'd\",\n",
       " \"they'd\",\n",
       " \"you'd\",\n",
       " 'off',\n",
       " 'had',\n",
       " \"what's\",\n",
       " 'www',\n",
       " 'when',\n",
       " 'themselves',\n",
       " 'but',\n",
       " 't']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Get the default English stop words from CountVectorizer\n",
    "default_stopwords = CountVectorizer(stop_words='english').get_stop_words()\n",
    "\n",
    "all_stopwords = list(default_stopwords)\n",
    "# Combine the default and custom stop words\n",
    "for i in stopwords:\n",
    "    if i not in all_stopwords:\n",
    "        all_stopwords.append(i)\n",
    "\n",
    "# all_stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install pyldavis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, nltk, spacy, gensim\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pprint import pprint\n",
    "# Plotting tools\n",
    "import matplotlib.pyplot as plt\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['don', 'think', 'it', 'works', 'like', 'that'], ['do', 'we', 'have', 'handicap', 'add', 'on', 'to', 'our', 'governments', 'student', 'money', 'but', 'if', 'you', 'have', 'it', 'you', 'can', 'only', 'earn', 'an', 'amount', 'of', 'money', 'on', 'the', 'side', 'and', 'you', 'only', 'get', 'it', 'as', 'long', 'as', 'you', 're', 'student', 'obviously']]\n"
     ]
    }
   ],
   "source": [
    "# Tokenization is the act of breaking up a sequence of strings into pieces such as words, keywords, phrases, symbols and other elements called tokens. \n",
    "# Tokens can be individual words, phrases or even whole sentences. In the process of tokenization, some characters like punctuation marks are discarded. \n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence))) \n",
    "data_words = list(sent_to_words(data.text))\n",
    "\n",
    "# append 'words' column to dataset\n",
    "data.loc[:, 'words'] = data_words\n",
    "print(data_words[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma.\n",
    "# The advantage of this is, we get to reduce the total number of unique words in the dictionary. \n",
    "# As a result, the number of columns in the document-word matrix (created by CountVectorizer in the next step) will be denser with lesser columns. \n",
    "# You can expect better topics to be generated in the end.\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']): \n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append(\" \".join([token.lemma_ if token.lemma_ not in ['-PRON-'] else '' for token in doc if token.pos_ in allowed_postags]))\n",
    "    return texts_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['think work', 'add government student money earn amount money side get student']\n"
     ]
    }
   ],
   "source": [
    "# Initialize spacy ‘en’ model, keeping only tagger component (for efficiency)\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "# Do lemmatization keeping only Noun and Verb - these are the parts of speech that usually reflect topics\n",
    "data_lemmatized = lemmatization(data.words, allowed_postags=['NOUN', 'VERB']) \n",
    "data.loc[:, 'lemmas'] = data_lemmatized\n",
    "print(data_lemmatized[:2])\n",
    "\n",
    "# After this pre-processing, the post text is represented as a collection of words (= bag of words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 522377 entries, 0 to 522376\n",
      "Data columns (total 14 columns):\n",
      " #   Column              Non-Null Count   Dtype \n",
      "---  ------              --------------   ----- \n",
      " 0   id                  522377 non-null  object\n",
      " 1   main_submission_id  522377 non-null  object\n",
      " 2   comment_parent_id   522377 non-null  object\n",
      " 3   subreddit           522377 non-null  object\n",
      " 4   post_type           522377 non-null  object\n",
      " 5   text                522377 non-null  object\n",
      " 6   datetime            522377 non-null  object\n",
      " 7   month               522377 non-null  int64 \n",
      " 8   year                522377 non-null  int64 \n",
      " 9   text_length         522377 non-null  int64 \n",
      " 10  language            522377 non-null  object\n",
      " 11  language_ft         522377 non-null  object\n",
      " 12  words               522377 non-null  object\n",
      " 13  lemmas              522377 non-null  object\n",
      "dtypes: int64(3), object(11)\n",
      "memory usage: 55.8+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>main_submission_id</th>\n",
       "      <th>comment_parent_id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>post_type</th>\n",
       "      <th>text</th>\n",
       "      <th>datetime</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>text_length</th>\n",
       "      <th>language</th>\n",
       "      <th>language_ft</th>\n",
       "      <th>words</th>\n",
       "      <th>lemmas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>is4ft9s</td>\n",
       "      <td>y2q46p</td>\n",
       "      <td>t3_y2q46p</td>\n",
       "      <td>autism</td>\n",
       "      <td>comment</td>\n",
       "      <td>I don t think it works like that</td>\n",
       "      <td>2022-10-13 05:58:56</td>\n",
       "      <td>10</td>\n",
       "      <td>2022</td>\n",
       "      <td>32</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>[don, think, it, works, like, that]</td>\n",
       "      <td>think work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is4gwqj</td>\n",
       "      <td>y2q46p</td>\n",
       "      <td>t3_y2q46p</td>\n",
       "      <td>autism</td>\n",
       "      <td>comment</td>\n",
       "      <td>I do we have handicap add on to our government...</td>\n",
       "      <td>2022-10-13 06:12:48</td>\n",
       "      <td>10</td>\n",
       "      <td>2022</td>\n",
       "      <td>189</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>[do, we, have, handicap, add, on, to, our, gov...</td>\n",
       "      <td>add government student money earn amount money...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id main_submission_id comment_parent_id subreddit post_type  \\\n",
       "0  is4ft9s             y2q46p         t3_y2q46p    autism   comment   \n",
       "1  is4gwqj             y2q46p         t3_y2q46p    autism   comment   \n",
       "\n",
       "                                                text             datetime  \\\n",
       "0                   I don t think it works like that  2022-10-13 05:58:56   \n",
       "1  I do we have handicap add on to our government...  2022-10-13 06:12:48   \n",
       "\n",
       "   month  year  text_length language language_ft  \\\n",
       "0     10  2022           32       en          en   \n",
       "1     10  2022          189       en          en   \n",
       "\n",
       "                                               words  \\\n",
       "0                [don, think, it, works, like, that]   \n",
       "1  [do, we, have, handicap, add, on, to, our, gov...   \n",
       "\n",
       "                                              lemmas  \n",
       "0                                         think work  \n",
       "1  add government student money earn amount money...  "
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(analyzer='word',\n",
    "                     max_df=0.9,                        # maimum required occurences of a word \n",
    "                     min_df=2,                          # minimum required occurences of a word \n",
    "                     stop_words=all_stopwords,          # remove stop words\n",
    "                     lowercase=True,                    # convert all words to lowercase\n",
    "                     token_pattern='[a-zA-Z0-9]{3,}',   # num chars > 3\n",
    "                     max_features=50000,                # max number of uniq words\n",
    "                    )\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no need to do train/validate/test split as it is unsupervised learning\n",
    "# vectorize all text dataset\n",
    "data_vectorized = cv.fit_transform(data.lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<522377x37574 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 7156031 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 522377 text rows\n",
    "# each text/document row is represented by a 37574 dimentions vector (dataset has 37574 features)\n",
    "data_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the vectors to a DataFrame\n",
    "vectors = pd.DataFrame(data_vectorized.toarray(), columns=cv.get_feature_names_out())\n",
    "\n",
    "# Concatenate the original DataFrame with the vectors DataFrame\n",
    "data = pd.concat([data, vectors], axis=1)\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documentation: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html\n",
    "# learning_decayfloat, default=0.7\n",
    "# learning_method{‘batch’, ‘online’}, default=’batch’\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "max_iter = 10\n",
    "\n",
    "LDA = LatentDirichletAllocation(n_components=10,            # number of topics\n",
    "                                max_iter=max_iter,          # \n",
    "                                batch_size=512,             # number docs in each learning iteration (as there are 10 iter, max 5120 of the docs will be seen when building the model)\n",
    "                                random_state=7,\n",
    "                                learning_method='online',\n",
    "                                evaluate_every = -1,        # compute perplexity every n iters, default: Don't\n",
    "                                n_jobs = -1,                # Use all availble CPUs\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LatentDirichletAllocation(batch_size=512, learning_method='online', n_jobs=-1,\n",
      "                          random_state=7)\n"
     ]
    }
   ],
   "source": [
    "LDA.fit(data_vectorized)\n",
    "print(LDA)  # Model attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Likelihood:  -64526052.89568952\n",
      "Perplexity:  1648.8671941932269\n",
      "{'batch_size': 512,\n",
      " 'doc_topic_prior': None,\n",
      " 'evaluate_every': -1,\n",
      " 'learning_decay': 0.7,\n",
      " 'learning_method': 'online',\n",
      " 'learning_offset': 10.0,\n",
      " 'max_doc_update_iter': 100,\n",
      " 'max_iter': 10,\n",
      " 'mean_change_tol': 0.001,\n",
      " 'n_components': 10,\n",
      " 'n_jobs': -1,\n",
      " 'perp_tol': 0.1,\n",
      " 'random_state': 7,\n",
      " 'topic_word_prior': None,\n",
      " 'total_samples': 1000000.0,\n",
      " 'verbose': 0}\n"
     ]
    }
   ],
   "source": [
    "# evaluate model performance\n",
    "\n",
    "# Log Likelyhood: Higher the better\n",
    "print(\"Log Likelihood: \", LDA.score(data_vectorized))\n",
    "\n",
    "# Perplexity: Lower the better. Perplexity = exp(-1. * log-likelihood per word)\n",
    "# perplexity might not be the best measure to evaluate topic models because it doesn’t consider the context and semantic associations between words.\n",
    "print(\"Perplexity: \", LDA.perplexity(data_vectorized))\n",
    "\n",
    "# See model parameters\n",
    "pprint(LDA.get_params())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the vocab of the words\n",
    "vocab_size = len(cv.get_feature_names_out())\n",
    "print('Number of words in vocab.: ', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the most common words per topic = words that have the highes probabilities of belonging to a topic\n",
    "# argsort() gets index positions sorted from least to greatest\n",
    "# top 10 = last 10 vlaues of argsort()\n",
    "# code reference documentation:\n",
    "# https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html#sphx-glr-auto-examples-applications-plot-topics-extraction-with-nmf-lda-py\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_top_words(model, feature_names, n_top_words, title):\n",
    "    fig, axes = plt.subplots(4, 5, figsize=(30, 32), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_features_ind = topic.argsort()[-n_top_words:]\n",
    "        top_features = feature_names[top_features_ind]\n",
    "        weights = topic[top_features_ind]  # \n",
    "\n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_features, weights, height=0.7)\n",
    "        ax.set_title(f\"Topic {topic_idx +1}\", fontdict={\"fontsize\": 30})\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n",
    "        for i in \"top right left\".split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "        fig.suptitle(title, fontsize=40)\n",
    "\n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the topics\n",
    "topics = LDA.components_\n",
    "\n",
    "# plot the top 10 words per topic\n",
    "feature_names = np.array(cv.get_feature_names_out())\n",
    "title = 'Top 10 Words per Topic'\n",
    "plot_top_words(LDA, feature_names, 10, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/andreeanistor/Desktop/MSprojectReddit2/topic_analysis_eng_posts.ipynb Cell 21\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/andreeanistor/Desktop/MSprojectReddit2/topic_analysis_eng_posts.ipynb#Y101sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m model \u001b[39m=\u001b[39m GridSearchCV(lda, param_grid\u001b[39m=\u001b[39msearch_params, cv\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, n_jobs\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, error_score\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/andreeanistor/Desktop/MSprojectReddit2/topic_analysis_eng_posts.ipynb#Y101sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# Do the Grid Search\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/andreeanistor/Desktop/MSprojectReddit2/topic_analysis_eng_posts.ipynb#Y101sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(data_vectorized)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/model_selection/_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    868\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[1;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    870\u001b[0m     )\n\u001b[1;32m    872\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[0;32m--> 874\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[1;32m    876\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    878\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1388\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1386\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1387\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1388\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/model_selection/_search.py:821\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    814\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[1;32m    815\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    816\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    817\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[1;32m    818\u001b[0m         )\n\u001b[1;32m    819\u001b[0m     )\n\u001b[0;32m--> 821\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    822\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    823\u001b[0m         clone(base_estimator),\n\u001b[1;32m    824\u001b[0m         X,\n\u001b[1;32m    825\u001b[0m         y,\n\u001b[1;32m    826\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[1;32m    827\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[1;32m    828\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[1;32m    829\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[1;32m    830\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[1;32m    831\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[1;32m    832\u001b[0m     )\n\u001b[1;32m    833\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[1;32m    834\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[1;32m    835\u001b[0m     )\n\u001b[1;32m    836\u001b[0m )\n\u001b[1;32m    838\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    839\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    840\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    841\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    842\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    843\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/joblib/parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1098\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretrieve()\n\u001b[1;32m   1099\u001b[0m \u001b[39m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m elapsed_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/joblib/parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    974\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, \u001b[39m'\u001b[39m\u001b[39msupports_timeout\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 975\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout))\n\u001b[1;32m    976\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    977\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39mget())\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/joblib/_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[39mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 567\u001b[0m     \u001b[39mreturn\u001b[39;00m future\u001b[39m.\u001b[39;49mresult(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m    568\u001b[0m \u001b[39mexcept\u001b[39;00m CfTimeoutError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    569\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/concurrent/futures/_base.py:453\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m    451\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__get_result()\n\u001b[0;32m--> 453\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_condition\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    456\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Use GridSearch to determine the best LDA model.\n",
    "# The most important tuning parameter for LDA models is n_components (number of topics).\n",
    "\n",
    "# Define Search Param\n",
    "search_params = {'n_components': [10, 15, 20], 'batch_size': [128, 512]}\n",
    "# Init the Model\n",
    "lda = LatentDirichletAllocation(max_iter=10,      \n",
    "                                random_state=7,\n",
    "                                learning_method='online',\n",
    "                                evaluate_every = -1,    # compute perplexity every n iters, default: Don't\n",
    "                                n_jobs = -1,            # Use all availble CPUs\n",
    "                              )\n",
    "# Init Grid Search Class\n",
    "model = GridSearchCV(lda, param_grid=search_params, cv=None, n_jobs=-1, error_score='raise', refit=True)\n",
    "# Do the Grid Search\n",
    "model.fit(data_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Model\n",
    "best_lda_model = model.best_estimator_\n",
    "# Model Parameters\n",
    "print(\"Best Model's Params: \", model.best_params_)\n",
    "# Log Likelihood Score\n",
    "print(\"Best Log Likelihood Score: \", model.best_score_)\n",
    "# Perplexity\n",
    "print(\"Model Perplexity: \", best_lda_model.perplexity(data_vectorized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# %pip install -U pip setuptools wheel\n",
    "# %pip install -U 'spacy[apple]'\n",
    "# !python3 -m spacy download en_core_web_sm\n",
    "# !python3 -m spacy validate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# %pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Deep learning pipeline\n",
    "\n",
    "pre-process text --> vectorize (dense embedings) --> deep learning Neural Net (hidden layers) --> output (here: topics)\n",
    "\n",
    "In the DL pipeline, the raw data (after pre-processing) is directly fed to a model. The model is capable of “learning” features from the data. Hence, these features are more in line with the task at hand, so they generally give improved performance. But, since all these features are learned via model parameters, the model loses interpretability. (Vajjala et al., 2020)\n",
    "\n",
    "<small>Vajjala, S., Majumder, B., Gupta, A., & Surana, H. (2020). Practical Natural Language Processing. O'Reilly Media, Inc.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERTopic\n",
    "'BERTopic generates document embedding with pre-trained transformer-based language models, clusters these embeddings, and finally, generates topic representations with the class-based TF-IDF procedure. BERTopic generates coherent topics and remains competitive across a variety of benchmarks involving classical models and those that follow the more recent clustering approach of topic modeling.' (Grootendorst, 2022)\n",
    "\n",
    "<small>Grootendorst, M. (2022). \"BERTopic: Neural topic modeling with a class-based TF-IDF procedure.\" arXiv:2203.05794v1 [cs.CL]. </small>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from sklearn.decomposition import LatentDirichletAllocation\n",
    "# import torch\n",
    "# from transformers import DistilBertTokenizer, DistilBertModel\n",
    "# from pytorch_pretrained_bert import BertTokenizer, BertModel"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
